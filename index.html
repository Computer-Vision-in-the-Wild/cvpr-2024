<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!--Open Graph Related Stuff-->
    <meta property="og:title" content="Workshop on Computer Vision in the Wild 2024" />
    <meta property="og:url" content="https://computer-vision-in-the-wild.github.io/cvpr-2024/" />
    <meta property="og:description" content="June 17 at CVPR 2024" />
    <meta property="og:description" content="CVPR 2024" />
    <meta property="og:site_name" content="Computer Vision in the Wild 2024" />
    <meta property="og:image" content="https://computer-vision-in-the-wild.github.io/cvpr-2024/static/cvpr2024/img/head_img/cvpr_2024_header.jpeg" />
    <meta property="og:image:url" content="https://computer-vision-in-the-wild.github.io/cvpr-2024/static/cvpr2024/img/head_img/cvpr_2024_header.jpeg" />
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Workshop on Computer Vision in the Wild 2024">
    <meta name="twitter:description" content="June 19 at CVPR 2024">
    <meta name="twitter:description" content="CVPR 2024">
    <meta name="twitter:image" content="https://computer-vision-in-the-wild.github.io/cvpr-2024/static/cvpr2024/img/head_img/cvpr_2024_header.jpeg">
    <title>Computer Vision in the Wild</title>
    <link rel="stylesheet" href="./static/css/foundation.css">
    <link rel="stylesheet" href="./static/css/main.css">
    <script src="./static/js/vendor/jquery.js"></script>
    <script src="./static/js/jquery-2.1.3.min.js"></script>

    <script type="text/javascript" src="./static/js/jquery.countdown.min.js"></script>
    <script type="text/javascript" src="./static/js/moment.min.js"></script>
    <script type="text/javascript" src="./static/js/moment-timezone-with-data.min.js"></script>

    <script type="text/javascript" src="./static/js/main-vqa.js"></script>
    <script type="text/javascript" src="./static/js/main-gqa.js"></script>
    <script type="text/javascript" src="./static/js/main-visdial.js"></script>
    <script type="text/javascript" src="./static/js/main-textvqa.js"></script>
    <script type="text/javascript" src="./static/js/main-textcaps.js"></script>
    <script type="text/javascript" src="./static/js/main-vizwiz.js"></script>

</head>
<style type="text/css">
.schedule table {
    -webkit-border-radius: 5px;
    -moz-border-radius: 5px;
    border-radius: 5px;
}

.schedule tr:hover {
    background-color: #D3D3D3;
}

.schedule img {
    max-height: 95px;
    max-width: 140px;
    margin-right: 2px;
    margin-top: 4px;
    margin-bottom: 4px;
    border-radius: 50%;
}

.abstract-content {
    display: none;
    padding: 5px;
}

.disclaimer {
    padding-left: 25px;
    text-align: left;
    font-size: 14px;
    line-height: 16px;
}
.disclaimer b {
    font-weight: bold !important;
}
</style>

<body class="off-canvas hide-extras" style="min-width:1300px; min-height:750px;">
    <header>
        <div class="row">
            <a href="https://computer-vision-in-the-wild.github.io/cvpr-2024"><img style="height: 160px; position:absolute; top:420px; right:26px;" src="./static/cvpr2024/img/head_img/cvpr_2024_logo1.jpeg" alt="logo" /></a>
            <!-- <h1><img style="height: 200px;" src="./static/eccv2022/img/cvinthewild_logo.jpg" alt="logo" /><br></h1> -->
            <br>
        </div>
    </header>
    <div class="contain-to-grid">
        <!-- <nav class="top-bar" data-topbar> -->
            <!-- <section class="top-bar-section"> -->
                <!-- Right Nav Section -->
                <!-- <ul class="right"> -->
                    <!-- <li><a href="index.html">Home</a></li> -->
                    <!-- <li><a href="people.html">People</a></li> -->
                    <!-- <li><a href="code.html">Code</a></li> -->
                    <!-- <li><a href="http://vqa.cloudcv.org/" onClick="ga('send', 'event', { eventCategory: 'Outgoing Link', eventAction: 'Demo', eventLabel: 'Demo'});">Demo</a></li> -->
                    <!-- <li class="has-dropdown"><a href="download.html">Download</a> -->
                        <!-- <ul class="dropdown"> -->
                            <!-- <li><a href="download.html">VQA v2</a></li> -->
                            <!-- <li><a href="vqa_v1_download.html">VQA v1</a></li> -->
                        <!-- </ul> -->
                    <!-- </li> -->
                    <!-- <li><a href="evaluation.html">Evaluation</a></li>
                    <li class="has-dropdown"><a href="challenge.html">Challenge</a>
                        <ul class="dropdown">
                            <li><a href="challenge.html">2021</a></li>
                            <li><a href="challenge_2020.html">2020</a></li>
                            <li><a href="challenge_2019.html">2019</a></li>
                            <li><a href="challenge_2018.html">2018</a></li>
                            <li><a href="challenge_2017.html">2017</a></li>
                            <li><a href="challenge_2016.html">2016</a></li>
                        </ul>
                    </li> -->
                    <!-- <li class="has-dropdown"><a href="http://visualqa.org/vqa_v2_teaser.html">Browse</a>
                        <ul class="dropdown">
                            <li><a href="http://visualqa.org/vqa_v2_teaser.html">VQA v2</a></li>
                            <li><a href="https://vqabrowser.cloudcv.org/">VQA v1</a></li>

                        </ul>
                    </li> -->
                    <!-- <li><a href="http://visualqa.org/visualize/">Visualize</a></li> -->
                    <!-- <li class="active has-dropdown"><a href="workshop.html">Workshop</a>
                        <ul class="dropdown"> -->
                            <!-- <li><a href="workshop.html"></a></li> -->
                            <!-- <li><a href="workshop_2020.html">2020</a></li>
                            <li><a href="workshop_2019.html">2019</a></li>
                            <li><a href="workshop_2018.html">2018</a></li>
                            <li><a href="workshop_2017.html">2017</a></li>
                            <li><a href="workshop_2016.html">2016</a></li> -->
                        <!-- </ul> -->
                    <!-- </li> -->
                    <!-- <li><a href="sponsors.html">Sponsors</a></li> -->
                    <!-- <li><a href="terms.html">Terms</a></li> -->
                    <!-- <li><a href="external.html">External</a></li> -->
                <!-- </ul> -->
            </section>
        </nav>
    </div>
    <section role="main" style="padding: 1em;">
        <div class="row">
            <p style="font-size:50px; color:black; font-weight: 50" align=center>The 3rd Workshop on Computer Vision in the Wild <br> 
                <span style="font-size:30px; color:gray; font-weight: 50" align=center> -- Building General-Purpose Assistants with Large Multimodal Models<br> </span>
                <span style="font-size:30px; color:gray; font-weight: 50" align=center>@ CVPR 2024, June 17 || Location: Arch 3B<br> </span>

<!--                  <span style="font-size:20px; color:gray; font-weight: 30" align=center>9:00am-6:00pm Israeli Time  ||  11:00pm (October 22)-8:00am Pacific Time  ||  2:00pm-11:00pm Beijing Time</span>   -->             
                <!-- <br> -->
                <!-- <br> -->
                <!-- <span style="font-size:20px; color:black; font-weight: 400" align=center>Zoom and Gatherly links on ECCV 2022 website: <a target="_blank" href="https://www.eventscribe.net/2021/2021CVPR/agenda.asp?startdate=6/19/2021&enddate=6/19/2021&BCFO=M&pfp=Workshops&mode=&tn=&cpftwo=&custwo=&pta=/">Link</a> <br> Navigate to: Workshops -> Sat, October 23 -> Search for "Computer Vision in the Wild" workshop entry</b></span> -->

            </p>




        <div class="row">
            <h1 style="font-size:30px; color:grey; font-weight: 200">Overview</h1>
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 400; text-align:left">

                A long-standing aspiration in artificial intelligence is to develop general-purpose assistants that can effectively follow users’ (multimodal) instructions to complete a wide range of real-world tasks. Recently, the community has witnessed a growing interest in developing foundation models with emergent abilities of multimodal understanding and generation in open-world tasks. While the recipes of using large language models (LLMs) such as ChatGPT to develop general-purpose assistants for natural language tasks have been proved effective, the recipes of building general-purpose, multimodal assistants for computer vision and vision-language tasks in the wild remain to be explored. Recent works show that learning from large-scale image-text data with human feedback in the loop is a promising approach to building transferable visual models that can effortlessly adapt to a wide range of downstream computer vision (CV) and multimodal (MM) tasks. For example, large multimodal models (LMM) such as Flamingo, GPT-4V, Gemini have demonstrated strong zero-shot transfer capabilities on many vision tasks in the wild. The open-source LMMs have also made significant progress, as demonstrated by OpenFlamingo, <a href="https://minigpt-4.github.io/"> MiniGPT4</a> and <a href="https://llava-vl.github.io/"> LLaVA</a>. These models are trained with visual instruction-following data, where human intents are represented in natural language. On the other hand, interactive vision systems such as <a href="https://github.com/facebookresearch/segment-anything">Segment Anything (SAM)</a> and <a href="https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once">SEEM</a> have also shown impressive segmentation performance on almost anything in the wild, where human intents are represented in visual prompts, such as click, bounding boxes and text. These vision models with language and multimodal interfaces are naturally open-vocabulary and even open-task models, showing superior zero-shot performance in various real-world scenarios.

                We host this “Computer Vision in the Wild (CVinW)” workshop, aiming to gather academic and industry communities to work on CV problems in real-world scenarios, focusing on the challenge of open-world visual task-level transfer. This CVPR 2024 CVinW workshop is a continuation of <a href="https://computer-vision-in-the-wild.github.io/cvpr-2023/"> CVPR 2023 CVinW Workshop</a> and
                <a href="https://computer-vision-in-the-wild.github.io/eccv-2022/"> ECCV 2022 CVinW Workshop</a>. For those who are new to this topic, please check out the  <a href="https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings/blob/main/README.md">CVinW Reading List</a>.
                
                <br>
                <br>
                
                The development of LMMs is an emerging new filed, with a vast research exploration space in data collection, modeling, evaluation and new application scenarios. There are many new evaluation and benchmarks merged to measure their performance from different aspects. To advocate established benchmarks to measure the progress, this workshop welcome authors different benchmarks to run indepedent challenges and report results. We highlight a few recent LMM evaluation toolkit / benchmarks:
                <ul style="font-size:15px;">

                    
                        <li style="margin-left:30px">
                            <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval">  <b> LMMs-Eval: The Evaluation Suite of Large Multimodal Models </b></a> 
                        </li>  
                        <li style="margin-left:30px">
                            <b> <a href="https://huggingface.co/spaces/WildVision/vision-arena">WildVision Arena: Evaluating Vision-Language Models in the Wild with Human Preferences </a></b>
                        </li>  
                        <li style="margin-left:30px">
                            <a href="https://huggingface.co/spaces/mlfoundations/VisIT-Bench-Leaderboard"> VisIT-Bench: Vision-Language Instruction Following </a>
                        </li>

                </ul>         

                </p>
            </div>
            <hr>
        </div>





<!-- Invited Speakers -->

 <!-- <div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Keynote Speaker</h1>
    <div class="team" id="people">
        <div class="row" style="display: flex; text-align:center; justify-content: center;">

            <div class="large-1 columns">
                <p></p>
            </div>


            <div class="large-4 columns">
                <a href="https://www.andrewng.org/"><img src="./static/cvpr2024/img/speakers/andrew-ng.png" class="speaker_picture" style="width:200px; height:200px;">
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Andrew Ng
                    <br> Founder of <a href="https://www.deeplearning.ai/">DeepLearning.AI</a>, <a href="https://landing.ai/">Landing AI</a>, General Partner at <a href="https://aifund.ai/">AI Fund</a>, Chairman and Co-Founder of <a href="https://www.coursera.org/">Coursera</a> and an Adjunct Professor at Stanford University. </p>
            </div>

 


            <div class="large-1 columns">
                <p></p>
            </div>


            <div class="large-1 columns">
                <p></p>
            </div>
        </div>

    <hr>
</div> -->



<!-- Invited Speakers -->
<!-- 
  <div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Invited Speakers/Panelists</h1>
        <div class="team" id="people">
            <div class="row" style="display: flex; text-align:center; justify-content: center;">

                <div class="large-2 columns">
                    <a href="https://www.cs.utexas.edu/users/grauman/"><img src="./static/cvpr2024/img/speakers/grauman.jpg"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Kristen Grauman
                        <br>University of Texas at Austin</p>
                </div>

                <div class="large-2 columns">
                    <a href="http://boqinggong.info/"><img src="./static/cvpr2024/img/speakers/boqing_gong.jpg"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Boqing Gong
                        <br>Google</p>
                </div>  

                <div class="large-2 columns">
                    <a href="https://web.eecs.umich.edu/~justincj/"><img src="https://web.eecs.umich.edu/~justincj/images/me-v2.jpg" class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Justin Johnson
                        <br>University of Michigan | FAIR</p>
                </div>
    
                <div class="large-2 columns">
                    <a href="https://sites.google.com/site/yinfeiyang"><img src="./static/eccv2022/img/yinfei_yang.jpeg" class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Yinfei Yang 
                        <br>Apple</p>
                </div>


                <div class="large-2 columns">
                        <a href="https://bryanplummer.com/"><img src="https://bryanplummer.com/profile.jpg"  class="speaker_picture" style="width:150px; height:150px;">
                            <br><br>
                        </a>
                        <p style="font-size:12px; font-weight: 200;">Bryan A. Plummer
                            <br>Boston University</p>
                </div>

                <div class="large-2 columns">
                    <a href="https://www.leizhang.org/"><img src="./static/cvpr2024/img/speakers/leizhang_square.jpg"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Lei Zhang
                        <br>International Digital Economy Academy (IDEA)</p>
                </div>

                <div class="large-1 columns">
                    <p></p>
                </div>
            </div>

            <div class="row" style="display: flex; text-align:center; justify-content: center;">



                <div class="large-2 columns">
                    <a href="https://liuziwei7.github.io/"><img src="./static/eccv2022/img/ziwei_liu.png"   class="speaker_picture" style="width:150px; height:150px;";>
                            <br><br>
                        </a>
                        <p style="font-size:12px; font-weight: 200;">Ziwei Liu
                            <br>NTU</p>
                </div>
                
                
                    <div class="large-2 columns">
                    <a href="https://blog.roboflow.com/author/jacob/"><img src="./static/cvpr2024/img/speakers/jacob.jpeg"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Jacob Solawetz
                        <br>Roboflow</p>
                </div>                            


                <div class="large-2 columns">
                    <a href="https://research.google/people/AneliaAngelova/"><img src="./static/cvpr2024/img/speakers/Anelia.gif"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Anelia Angelova
                        <br>Google Brain</p>
                </div>     

                    <div class="large-2 columns">
                    <a href="https://jiasenlu.github.io/"><img src="https://jiasenlu.github.io/images/profile.jpeg"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Jiasen Lu
                        <br>Allen Instutite for AI</p>
                </div>                        


                <div class="large-2 columns">
                    <a href="https://www.cs.cmu.edu/~katef/"><img src="https://www.cs.cmu.edu/~katef/images/stairs.png" target="_blank"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Katerina Fragkiadaki 
                        <br>CMU</p>
                </div>                        
    
                <div class="large-2 columns">
                    <a href="https://faculty.cc.gatech.edu/~dbatra/"><img src="./static/cvpr2024/img/speakers/dhruv_batra.jpg"  class="speaker_picture" style="width:150px; height:150px;">
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Dhruv Batra
                        <br>Georgia Tech | FAIR</p>
                </div>     

                <div class="large-1 columns">
                    <p></p>
                </div>
                    
            </div>

        <hr>
    </div> -->


    <div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Schedule (June 17th, Monday)</h1>

        <div id="content-2">
            <div class="schedule">
                <table width="100%">

                    <tr>
                        <td width=250>
                            <center><b>9:15 AM - 9:30 AM PT</b></center> 
                        </td>
                        <td width=300>
                            <center><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/09/20191101_170350330_iOS-4.jpg"></center>
                        </td>
                        <td style="padding-left:8px"><b>Welcome</b>
                            <a href="https://www.microsoft.com/en-us/research/people/jfgao/"><br>Jianfeng Gao - Microsoft Research</a>
                        </td>
                    </tr>



                    <tr>
                        <td colspan="3">
                        <center>
                        <br>
                        <em>
                        Morning Session On-Site Chair: <a href="https://maureenzou.github.io/">Xueyan Zou (UW Madison)</a>
                        </em>
                        <br><br>
                        </center>
                        </td>
                    </tr>

                    <tr>
                        <td width=250>
                            <center> <b>9:30 AM - 10:00 AM PT</b></center> 
                        </td>
                        <td width=300>
                            <center><img src="static/cvpr2024/img/speakers/profile_xiaolong.jpeg"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b>Invited Talk</b>
                            <br>
                            <a href="https://xiaolonw.github.io/"><br>Xiaolong Wang - USCD </a>  | 
                            <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Xiaolong Wang is an Assistant Professor in the ECE department at the University of California, San Diego. He received his Ph.D. in Robotics at Carnegie Mellon University. His postdoctoral training was at the University of California, Berkeley. His research focuses on the intersection between computer vision and robotics. His specific interest lies in learning 3D and dynamics representations from videos and physical robotic interaction data. These comprehensive representations are utilized to facilitate the learning of human-like robot skills, with the goal of generalizing the robot to interact effectively with a wide range of objects and environments in the real physical world. He is the recipient of the NSF CAREER Award, Intel Rising Star Faculty Award, and Research Awards from Sony, Amazon, Adobe, and Cisco.
                            </div>
                            <br> <br>
                            <b>Spatial Perception and Control in the Wild</b>
                            <br>
                            <!-- <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                In this talk we will look at several approaches for leveraging vision and language models for the purposes of object detection in the wild: F-VLM, RO-ViT and FindIt. Furthermore, I will introduce our scaled vision-language models (e.g. PaLI and PaLI-X) which have object detection capabilities. And, lastly, I will present our newest work, MaMMUT, which is a much smaller vision-language model, but capable of many vision-language tasks, including image-text and text-image retrieval, open-vocabulary object detection, video question and answering, video captioning, and others. 
                            </div> -->
                            

                        </td>
                    </tr>

                    <tr>
                        <td width=250>
                            <center> <b>10:00 AM - 10:30 AM PT</b></center> 
                        </td>
                        <td width=300>
                            <center> 
                                <h1 style="font-size:16px; color:#ffa200; font-weight: 500">Spotlight Paper Presentations
                                <img src="https://i.etsystatic.com/43871135/r/il/59ec81/4988653779/il_600x600.4988653779_9o7s.jpg" class=" style=" width="80" height="100" ;>
                                </h1> 
                                <b></b>
                            </center>
                        </td>
                        <td style="padding-left:8px">
                            <b></b>
                            * <a href="https://arxiv.org/abs/2404.12390"> BLINK: Multimodal Large Language Models Can See but Not Perceive  
                            </a> 
                            <br>
                            <a href="https://arxiv.org/abs/2403.14783"> 
                            * Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering
                            </a> 
                            <br>
                            <a href="https://arxiv.org/abs/2402.13254">
                            * CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples
                            </a>
                            <br>
                            <a href="https://arxiv.org/abs/2304.02364">
                            * What’s in a Name? Beyond Class Indices for Image Recognition </a>
                            <br>
                            <a href="https://arxiv.org/abs/2312.00784">
                            * ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts </a>
                            <br>
                            * LIT: Large Language Model Driven Intention Tracking for Proactive Human-Robot Collaboration - A Robot Sous-Chef Application
                            <br>
                            <br>
                        </td>
                    </tr>


                    <tr>
                        <td width=250>
                            <center> <b>10:30 AM - 11:00 AM PT</b></center> 
                        </td>
                        <td width=300>
                            <center><img src="https://jmhessel.com/main_photo.jpg"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b>Invited Talk</b>
                            <br>
                            <a href="https://jmhessel.com/"><br> Jack Hessel</a> - <a href="https://samaya.ai/ "> Samaya AI </a>  | 
                            <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Jack Hessel is a researcher at Samaya AI, a knowledge discovery startup. Previously, he was a postdoc+research scientist at the Allen Institute for AI, and before that, he earned a PhD from Cornell. His work, which spans language processing, machine learning, and computer vision, has been recognized with several awards, including ACL 2023 best paper and EMNLP 2023 outstanding paper.
                            </div>

                            <br> <br>
                            <b>Visual Details Don’t Matter (Until They Do)</b>
                            <br>
                            <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Visual details are unconsciously filtered by human attention: most things simply aren’t salient most of the time. But, what happens when small details *become* salient (as they often do), e.g., for a task, for a conversation, etc.? While humans can allocate directed, conscious perception with high fidelity, in this talk, I’ll highlight recent work demonstrating surprisingly simple cases where large vision+language models fall short. Along the way, we’ll encounter a paradox, a curse, and potential paths forward.
                            </div>

                        </td>
                    </tr> 
                    <tr>
                        <td width=250>
                            <center> <b>11:00 AM - 11:30 AM PT</b></center> 
                        </td>


                        <td width=300>
                            <center><img src="./static/eccv2022/img/ziwei_liu.png"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b style="color:#ffa200; font-weight: 500">Invited Talk: Evaluation of LMMs</b>
                            <br>
                            <a href="https://liuziwei7.github.io/"><br>Ziwei Liu - Nanyang Technological University</a> | 
                            <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Prof. Ziwei Liu is currently an Assistant Professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurIPS, ICLR, SIGGRAPH, TPAMI, TOG and Nature - Machine Intelligence. He is the recipient of Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award, CVPR Best Paper Award Candidate, WAIC Yunfan Award, ICBS Frontiers of Science Award and MIT Technology Review Innovators under 35 Asia Pacific. He serves as an Area Chair of CVPR, ICCV, ECCV, NeurIPS and ICLR, as well as an Associate Editor of IJCV.
                            </div>
                            <br>
                            <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval"><br> <b>LMMs-Eval: The Evaluation Suite of Large Multimodal Models </b></a> 
                            <br>
                            <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMs-Eval, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMs-Eval offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMs-Eval Lite, a pruned evaluation set that emphasizes both coverage and efficiency. Additionally, we present LiveBench that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs.
                            </div>                            
                            

                        </td>
                    </tr>                   

                    <tr>
                        <td width=250>
                            <center>  <b>11:30 AM - 12:00 PM PT</b></center> 
                        </td>
                        <td width=300>
                            <center><img src="https://www.gshi.me/Guanya_Commencement.jpg"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b>Invited Talk</b>
                            <br>
                            <a href="https://www.gshi.me/"><br> Guanya Shi - Carnegie Mellon University</a> | 
                            <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Guanya Shi is an Assistant Professor at the Robotics Institute at Carnegie Mellon University (CMU). He completed his Ph.D. in Control and Dynamical Systems in 2022 from Caltech. Before joining CMU, he was a postdoctoral scholar at the University of Washington. He is broadly interested in the intersection of machine learning and control theory, spanning the entire spectrum from theory and foundation, algorithm design, to real-world agile robotics. Guanya was the recipient of several awards, including the Simoudis Discovery Prize and the Ben P.C. Chou Doctoral Prize from Caltech, and the Rising Star in Data Science. Guanya is an Associate Editor of IEEE Robotics and Automation Letters.
                            </div>
                            <br><br>  
                            <a href="https://omni.human2humanoid.com/"><b>Unifying Semantic and Physical Intelligence for Generalist Humanoid Robots</b> </a>
                            <br>
                            <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Humanoid robots offer two unparalleled advantages in general-purpose embodied intelligence. First, humanoids are built as generalist robots that can potentially do all the tasks humans can do in complex environments. Second, the embodiment alignment between humans and humanoids allows for the seamless integration of human cognitive skills with versatile humanoid capabilities. To build generalist humanoids, there are three critical aspects of intelligence: (1) Semantic intelligence (how the robot understands the world and reasons); (2) Physical/Motion intelligence (locomotion and manipulation skills); and (3) Mechanical/Hardware intelligence (how the robot actuates and senses). In this talk, I will present some recent works (H2O, OmniH2O, ABS) that aim to unify semantic and physical intelligence for humanoid robots. In particular, H2O and OmniH2O provide a universal and dexterous interface that enables diverse human control (e.g., VR, RGB) and autonomy (e.g., using imitation learning or VLMs) methods for humanoids, and ABS provides safety guarantees for agile vision-based locomotion control.
                            </div>                            
                            
                        </td>
                    </tr>
                   


                    <tr>
                        <td colspan="3">
                        <center>
                        <br>
                        <em>
                        Afternoon Session On-Site Chair: <a href="https://laoreja.github.io/"> Haotian Liu (UW Madison) </a>
                        </em>
                        <br><br>
                        </center>
                        </td>
                    </tr>                    

                    <tr>
                        <td width=250>
                            <center> <b>1:30 PM - 2 PM PT</b></center> 
                        </td>


                        <td width=300>
                            <center><img src="./static/cvpr2024/img/speakers/Zhe_new2.jpeg"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b>Invited Talk</b>
                            <br>
                            <a href="https://zhegan27.github.io/"><br>Zhe Gan - Apple</a> | 
                            <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Zhe Gan is a Research Scientist and Manager at Apple, where he focuses on developing large-scale vision and multimodal foundation models. Prior to his tenure at Apple, he was a principal researcher at Microsoft Azure AI. He earned his Ph.D. from Duke University in 2018. He has consistently served as an Area Chair at leading AI conferences, including NeurIPS, ICML, ICLR, CVPR, ECCV, ACL, NAACL, and EMNLP. He has also been honored with the Best Student Paper Honorable Mention Awards at CVPR 2021 and WACV 2021.
                            </div>
                            <br> <br>
                            <b>  <a href="https://arxiv.org/abs/2403.09611/"> MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training</a>  </b>
                            <br>
                            <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                In this talk, I will discuss building performant Multimodal Large Language Models (MLLMs). In particular, I will discuss the lessons we have learned in developing MM1, a family of multimodal models, including both dense variants up to 30B and mixture-of-experts (MoE) variants up to 64B, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. If time permits, I will also briefly mention our Ferret model family, including the most recent Ferret-UI model.
                            </div>                            

                        </td>
                    </tr>
                    <tr>
                        <td width=250>
                            <center> <b>2:00 PM - 2:30 PM PT</b></center> 
                        </td>
                        <td width=300>
                            <center><img src="https://people.csail.mit.edu/ludwigs/images/me_1024px.jpg"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b>Invited Talk</b>
                            <br>
                            <a href="https://people.csail.mit.edu/ludwigs/"><br>Ludwig Schmidt</a>
                            <br>
                        </td>
                    </tr>
                    <tr>
                        <td width=250>
                            <center> <b>2:30 PM - 3:30 PM PT</b></center> 
                        </td>
                        <td width=300>
                            <center> 
                                <h1 style="font-size:16px; color:#ffa200; font-weight: 500">Poster Session (Afternoon Break) 
                                <img src="./static/eccv2022/img/icons/coffee_time.png" class=" style=" width="80" height="100" ;>
                                </h1> 
                                <b></b>
                            </center>
                        </td>
                        <td style="padding-left:8px">
                            Poster boards: #1 - #15 (Location: Arch 4E)
                            <br>
                        </td>
                    </tr>
                    <tr>
                        <td width=250>
                            <center> <b>3:30 PM - 4:00 PM PT</b></center> 
                        </td>
                        <td width=300>
                            <center><img src="./static/cvpr2024/img/speakers/yongjaelee-Nov2019.jpg"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b>Invited Talk</b>
                            <br>
                            <a href="https://pages.cs.wisc.edu/~yongjaelee/"><br>Yong Jae Lee - University of Wisconsin-Madison</a> |
                            <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Yong Jae Lee is an Associate Professor in the CS Dept at the University of Wisconsin-Madison. His research interests are in computer vision and machine learning, with a focus on creating robust visual recognition systems that can learn to understand the visual world with minimal human supervision. He is a recipient of several awards including the Army Research Office Young Investigator Award, NSF CAREER Award, Most Innovative Award at the COCO Object Detection Challenge ICCV 2019, and the Best Paper Award at BMVC 2020.
                            </div>

                             <br><br> 
                            <b>Building Steerable Generalist Multimodal Models</b>
                            <br> 
                            <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                I'll present how to build steerable large multimodal (vision-language) models that can understand human instructions and solve a variety of visual understanding tasks. I'll start by talking about the LLaVA series, and then discuss ways to make LLaVA understand visual prompts.  I'll also briefly discuss how to make it more personalized, and more adaptable to varying image complexities.  I'll conclude with a discussion on limitations and next steps.
Relevant project pages: <a href="https://llava-vl.github.io/">https://llava-vl.github.io/</a>, <a href="https://vip-llava.github.io/">https://vip-llava.github.io/</a>, <a href="https://thaoshibe.github.io/YoLLaVA/">https://thaoshibe.github.io/YoLLaVA/</a>
                            </div>                          
                            
                            
                        </td>
                    </tr>
                    <tr>
                        <td width=250>
                            <center> <b>4:00 PM - 4:30 PM PT</b></center> 
                        </td>
                        <td width=300>
                            <center><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=pcmr6GMAAAAJ&citpid=4"></center>
                        </td>
                        <td style="padding-left:8px">
                            <b style="color:#ffa200; font-weight: 500">Invited Talk: Evaluation of LMMs</b>
                            <br>
                            <a href="https://yujielu10.github.io/"><br>Yujie Lu</a> |
                            <b>Bio</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Yujie Lu is a third-year PhD student at the University of California, Santa Barbara, working with Prof. William Wang. Her research interests include vision and language models, as well as evaluation and benchmarking. Yujie interned at MSR, AWS AI, and Meta FAIR. She was honored to be elected as the Robert Noyce Fellow. Her work has been published in conferences such as ICCV, ICLR, NeurIPS, EMNLP, NAACL, CoRL, and received the CHI 2023 Best Paper Award. She also co-organized SoCalNLP 2022.    
                            </div>

                            <br><br>
                            <b> <a href="https://huggingface.co/spaces/WildVision/vision-arena">WildVision Arena: Evaluating Vision-Language Models in the Wild with Human Preferences </a></b>
                            <br>
                            <b>Abstract</b> <a class="abstract-toggle">[Expand]</a>
                            <div class="abstract-content">
                                Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar. Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.
                            </div>                            
                            
                        </td>
                    </tr>                       
        </tr>
 
                    
                </table>
            </div>
            <hr>
        </div>
    </div>



    

    <!-- <div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Accepted Papers</h1>
        <div class="large-12 columns" style="text-align:left;">
            <p style="font-size:15px; font-weight: 400; text-align:left">
                <ul style="font-size:15px;">
                    The following papers were accepted to the CV in the Wild Workshop (Poster boards: #135-#142):
                    <br>
                    <br>
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/5/CameraReady/cvinw_camera_ready.pdf>SITA: Single Image Test-time Adaptation</a></b> Ansh Khurana (Stanford University)*; Sujoy Paul (Google Research); Piyush Rai (IIT Kanpur); Soma Biswas (Indian Institute of Science, Bangalore); Gaurav Aggarwal (Google)</li> 
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/6/CameraReady/camera_ready.pdf>Perceptual Grouping in Contrastive Vision-Language Models</a></b> Kanchana N Ranasinghe (Stony Brook University)*; Brandon S McKinzie (Apple); Sachin Ravi (Princeton University); Yinfei Yang (Apple); Alexander Toshev (Apple); Jonathon Shlens (Google)</li>
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/7/CameraReady/vipergpt4page_cameraready.pdf>ViperGPT: Visual Inference via Python Execution for Reasoning</a></b> Dídac Surís (Columbia University); Sachit Menon (Columbia University)*; Carl Vondrick (Columbia University)</li>
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/8/CameraReady/2024139829.pdf>From Coarse to Fine-grained Concept based Discrimination for Phrase Detection</a></b> Maan Qraitem (Boston University)*; Bryan Plummer (Boston University)</li>
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/10/CameraReady/11_camera_ready.pdf>A Robust Likelihood Model for Novelty Detection</a></b> Ranya Almohsen (West Virginia University )*; Shivang A Patel (West Virginia University); Donald adjeroh (West Virginia University); Gianfranco Doretto (West Virginia University)</li>
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/12/CameraReady/CVinW_ESS_ID12.pdf>ESS: Learning Event-based Semantic Segmentation from Still Images</a></b> Zhaoning Sun (ETH Zürich); Nico Messikommer (University of Zurich & ETH Zurich)*; Daniel Gehrig (University of Zurich & ETH Zurich); Davide Scaramuzza (University of Zurich & ETH Zurich, Switzerland)</li>
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/13/CameraReady/CVinW__23.pdf>FixPNet: Attention Guided Fixation Map Prediction With Explicit Image Priors</a></b> Rakesh Radarapu (Samsung R & D)*; Sudha Velusamy (Samsung India); Anandavardhan Hegde (Samsung R & D); Narayan Kothari (Samsung R&D Institute, Bangalore, India)</li>
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/14/CameraReady/14.pdf>MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge</a></b> Wei Lin (Graz University of Technology)*; Leonid Karlinsky (IBM-Research); Nina Shvetsova (Goethe University Frankfurt); Horst Possegger (Graz University of Technology); Mateusz Kozinski (ICG TUGRAZ); Rameswar Panda (MIT-IBM Watson AI Lab); Rogerio Feris (MIT-IBM Watson AI Lab, IBM Research); Hilde Kuehne (Goethe University Frankfurt); Horst Bischof (Graz University of Technology)                    </li>                  
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/16/CameraReady/ImbaGCD_CVPR_Workshop.pdf>ImbaGCD: Imbalanced Generalized Category Discovery</a></b> Ziyun Li (Hasso Plattner Institute)*; Ben Dai (The Chinese University of Hong Kong); Furkan F Simsek (Hasso Plattner Institute); Meinel Christoph (Hasso Plattner Institute, Potsdam Germany); Haojin Yang (Hasso-Plattner-Institut für Digital Engineering gGmbH)                    </li> 
                    <li style="margin-left:30px"> <b><a href=./static/cvpr2024/accepted_papers/18/CameraReady/CVinW2024_FoodSeg_CR.pdf>Transferring Knowledge for Food Image Segmentation using Transformers and Convolutions</a></b> Grant Sinha (University of Waterloo)*; Krish Parmar (University of Waterloo); Hilda Azimi (NRC); Chi-en A Tai (University of Waterloo); Yuhao Chen (University of Waterloo); Alexander Wong (University of Waterloo); PENGCHENG XI (National Research Council Canada)                    </li> 
                    
    
                    <br>

    

                    </div>
    
                </ul>    

            </p>
        </div>
    </div>  -->

    <div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Accepted Papers</h1>
        <div class="large-12 columns" style="text-align:left;">
            <p style="font-size:15px; font-weight: 400; text-align:left">
                <ul style="font-size:15px;">
                    The following papers are accepted to the CVinW Workshop. Congratulations to all authors!
                    <br>
                    <br>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering</a></b> Bowen Jiang (University of Pennsylvania)*; Zhijun Zhuang (University of Pennsylvania); Shreyas Skandan Shivakumar (University of Pennsylvania); Dan Roth (UPenn); Camillo Jose Taylor (University of Pennsylvania)</li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">LLMGeo: Benchmarking Large Language Models on Image Geolocation In-the-wild</a></b> Zhiqiang  Wang (Florida Atlantic University ); Dejia Xu (University of Texas at Austin)*; Rana Muhammad Shahroz Khan (Vanderbilt University); Yanbin Lin (Florida Atlantic University); Zhiwen Fan (University of Texas at Austin); Xingquan Zhu (Florida Atlantic University)</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples</a></b>Jianrui Zhang (University of Wisconsin-Madison)*; Mu Cai (University of Wisconsin-Madison); Tengyang Xie (University of Wisconsin-Madison); Yong Jae Lee (University of Wisconsin-Madison)</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts</a></b> Mu Cai (University of Wisconsin-Madison)*; Haotian Liu (University of Wisconsin-Madison); Dennis Park (Cruise LLC); Siva Karthik Mustikovela (Uni Heidelberg); Gregory P Meyer (Cruise); Yuning Chai (Alphabet); Yong Jae Lee (University of Wisconsin-Madison)</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">Multimodal Procedural Planning via Dual Text-Image Prompting</a></b> Yujie Lu (University of California, Santa Barbara)*; Pan Lu (University of California, Los Angeles); Zhiyu Chen (CMU); Wanrong Zhu (University of California, Santa Barbara); Xin Eric Wang (University of California, Santa Cruz); William Yang Wang (UC Santa Barbara)</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">DSNet: A Novel Way to Use Atrous Convolutions in Semantic Segmentation</a></b> Zilu Guo (Anhui University, Institutes of Physical Science and Information Technology); Liuyang Bian (Anhui University, Institutes of Physical Science and Information Technology); Huang Xuan (Hefei Institutes of Physical Science, Chinese Academy of Sciences)*; Hu Wei (Hefei Institutes of Physical Science, Chinese Academy of Sciences); Jingyu Li (University of Science and Technology of China); Huasheng Ni (Hefei Institutes of Physical Science, Chinese Academy of Sciences)</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">Heron-Bench: A Benchmark for Evaluating Vision Language Models in Japanese</a></b> Yuichi Inoue (Turing Inc.)*; Kento Sasaki (Turing.Inc); Yuma Ochi (Turing In.c); Kazuki Fujii (Tokyo Institute of Technology); Kotaro Tanahashi (Turing Inc.); Yu Yamaguchi (Turing Inc.)</li>
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">GroPrompt: Efficient Grounded Prompting and Adaptation for Referring Video Object Segmentation</a></b> Ci-Siang Lin (National Taiwan University)*; I-Jieh Liu (National Taiwan University); Min-Hung Chen (NVIDIA); Chien-Yi Wang (NVIDIA); Sifei Liu (NVIDIA); Yu-Chiang Frank Wang (National Taiwan University)                   </li>                  
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">What’s in a Name? Beyond Class Indices for Image Recognition</a></b> Kai Han (The University of Hong Kong); Xiaohu Huang (The University of Hong Kong)*; Yandong Li (Google); Sagar Vaze (Visual Geometry Group, University of Oxford); Jie Li (The University of Hong Kong); xuhui jia (google)                   </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">HierGAN: GAN-Based Hierarchical Model for Combined RGB and Depth Inpainting</a></b> Ankan Dash (New Jersey Institute of Technology)*; Jingyi Gu (New Jersey Institute of Technology); Guiling  Wang (New Jersey Institute of Technology)                   </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">Investigating Reliable Question Decomposition for Vision-Language Tasks</a></b> Qian Yang (Mila)*; Weixiang Yan (University of California, Santa Barbara); Aishwarya Agrawal (University of Montreal, Mila, DeepMind)                  </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">HowToCaption: Prompting LLMs to Transform Video Annotations at Scale</a></b> Nina Shvetsova (Goethe University Frankfurt); Anna Kukleva (MPII)*; Xudong Hong (Saarland University); Christian Rupprecht (University of Oxford); Bernt Schiele (MPI Informatics); Hilde Kuehne (University of Bonn)                  </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">BLINK: Multimodal Large Language Models Can See but Not Perceive</a></b> Xingyu Fu (University of Pennsylvania)*; Yushi Hu (University of Washington); Bangzheng Li (University of California, Davis); Yu Feng (University of Pennsylvania); Haoyu Wang (University of Pennsylvania); Xudong Lin (Columbia University); Dan Roth (UPenn); Noah A Smith (University of Washington and Allen Institute for AI); Wei-Chiu Ma (Cornell University); Ranjay Krishna (University of Washington)                  </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations</a></b> Sri Harsha Dumpala (Dalhousie University/Vector Institute)*; Aman Jaiswal (Dalhousie University); Chandramouli Shama Sastry (Dalhousie University/Vector Institute); Evangelos Milios (Dalhousie University); Sageev Oore (Dalhousie University and Vector Institute); Hassan Sajjad (Dalhousie University)                 </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">LIT: Large Language Model Driven Intention Tracking for Proactive Human-Robot Collaboration - A Robot Sous-Chef Application</a></b> Zhe Huang (University of Illinois at Urbana-Champaign)*; John Pohovey (University of Illinois Urbana-Champaign); Ananya  Yammanuru  (University of Illinois at Urbana Champaign); Katherine Driggs-Campbell (University of Illinois at Urbana-Champaign)                  </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">CulturalVQA: Benchmarking Vision Language Models for Cultural Understanding</a></b> Shravan Nayak (Mila)*; Kanishk Jain (Mila); Karolina Stanczak (Mila - Quebec Artificial Intelligence Institute / McGill University); Md Rabiul Awal (Mila); Aishwarya Agrawal (University of Montreal, Mila, DeepMind)                </li> 
                    <li style="margin-left:30px"> <b><a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/">Segment and Recognize Anything at Any Granularity</a></b> Feng Li (Hong Kong Univerity of Science and Technology)*; Hao Zhang (hkust); Peize Sun (The University of Hong Kong); Xueyan Zou (university of wisconsin, madison); Shilong Liu (Tsinghua University); Chunyuan Li (Microsoft Research, Redmond); Jianwei Yang (Microsoft Research); Lei Zhang (International Digital Economy Academy (IDEA)); Jianfeng  Gao (Microsoft Research)               </li> 
                    <br>
                    <!-- We accept abstract submissions to our workshop. All submissions shall have maximally 8 pages (excluding references) following the ECCV 2022 author guidelines. All submissions will be reviewed by the Program Committee on the basis of technical quality, relevance to scope of the conference, originality, significance, and clarity. Ther review process is double-blind, and the accepted papers are non-archived in ECCV proceeding. -->
                    <!-- <br> <br> -->
    

                    </div>
    
                </ul>    

            </p>
        </div>
    </div>



<div class="row">
            <h1 style="font-size:30px; color:grey; font-weight: 200">Call for Papers</h1>
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 400; text-align:left">
                    Topics of interest include but are not limited to:
                    <ul style="font-size:15px;">
                        <li style="margin-left:30px">
                            LMMs: collection/curation/creation of pre-trainig and instruction-following data, alignment with human intents and modeling.
                        </li>   
                        <li style="margin-left:30px">
                            New metrics / benchmarks / datasets to evaluate LMM, task-level transfer and open-set visual recognition
                        </li>  
                        <li style="margin-left:30px">
                            Unified neural networks architectures and training objectives over different CV & MM tasks
                        </li>
                        <li style="margin-left:30px">
                            Tool use, external knoweldge and multimodal agents
                        </li>
                        <li style="margin-left:30px">
                            Open-set visual recognition methods, including classification, object detection, segmentation in images and videos
                        </li>
                        <li style="margin-left:30px">
                            Efficient large visual model adaptation methods, measured by #training samples (zero-shot and few-shot), #trainable parameters, throughput, training cost
                        </li>       
                        <li style="margin-left:30px">
                            Efficien prompting, inference and serving techniques at scale.
                        </li>            
          
        
                        <br>
                        We accept abstract submissions to our workshop. All submissions shall have maximally 8 pages (excluding references) following the CVPR 2024 author guidelines. All submissions will be reviewed by the Program Committee on the basis of technical quality, relevance to scope of the conference, originality, significance, and clarity. Ther review process is double-blind, and the accepted papers are NOT archived in CVPR proceeding.
                        <br> <br>
        
                        <div class="large-12 columns" style="text-align:left;">
                            <p style="font-size:15px; font-weight: 200; border-style: solid;
                                          border-width: 1px; text-align:justify; padding:5px; width:85%">
                                <code>
        
                                    <span style="display:inline-block; margin-left:5px;">
                                        <a href="https://cmt3.research.microsoft.com/CVinW2024"> <img src="./static/eccv2022/img/icons/icon_live.png" class=" style=" width="45" height="40" ;> </a>
                                    </span>
                                    <span style="width:85%; margin:5px; display:inline-block;">
                                        Workshop Paper Submission Portal: 
                        <a href="https://cmt3.research.microsoft.com/CVinW2024"> [CMT] </a></span>
                                    <span style="display:inline-block; margin-left:10px;"></span><br>
                                </code>
                            
                        </p>
                        </div>
                    </div>
                    <hr> 
</div>



<!-- <div class="row">
            <h1 style="font-size:30px; color:grey; font-weight: 200">Computer Vision in the Wild Challenges</h1>
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 400; text-align:left">          
        
        The two new challenges are developed:

                <table width="85%">
                <tr>
                    <td width=30>
                        <center> <span style="color:gray;"><b>Challenge</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Eval Datasets</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Eval Metrics</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Instructions</b></span> <br>
                    </td>                            
                    <td width=170>
                        <center> <span style="color:black;"><b>Make a Challenge Submission</b>
                            <img src="./static/eccv2022/img/icons/icon_live.png" class=" style=" width="50" height="40" ;>
                        </span> <br>
                    </td>                            

                </tr>
                <tr>
                    <td width=30>
                        <center> <span><b>SGinW</b></span></center>
                    </td>
                    <td width=150>
                        <center> <span><b>25 Image Segmentation Datasets</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><b>Zero, few, full-shot</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><a href="https://github.com/microsoft/X-Decoder/tree/seginw"> <img src="./static/eccv2022/img/icons/instruction.png" class="style=" width="50" height="50" ;> </a></span></center>
                    </td>     
                    <td width=170>
                        <center> <span><a href="https://eval.ai/web/challenges/challenge-page/1931/overview"> <img src="https://evalai.s3.amazonaws.com/media/logos/0b7a5a6d-cc4c-42d1-a155-c829ea036d51.jpg" class="home_team_picture style=" width="100" height="100" ;> </a></span></center>
                    </td>                                                        
                </tr>
                <tr>
                    <td width=30>
                        <center> <span><b>RF100</b></span></center>
                    </td>
                    <td width=150>
                        <center> <span><b>100 Object Detection Datasets</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><b>Zero, few, full-shot</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><a href="https://www.rf100.org/"> <img src="./static/eccv2022/img/icons/instruction.png" class=" style=" width="50" height="50" ;> </a></span></center>
                    </td>  
                    <td width=170>
                        <center> <span><a href="https://eval.ai/web/challenges/challenge-page/1973/overview"> <img src="https://evalai.s3.amazonaws.com/media/logos/678927bd-a0f9-44f6-b292-d3af66bf6e58.png" class="home_team_picture style=" width="100" height="100" ;> </a></span></center>
                    </td>  
                </tr>
                </tr>
            </table>                                 
                    
                    
        <ul style="font-size:15px;"> 
        The two existing challenges associated with this workshop: "Image Classification in the Wild" (ICinW) and "Object Detection in the Wild" (ODinW). We summarize their evaluation datasets and metrics in the table below.<br><br>
            <table width="85%">
                <tr>
                    <td width=30>
                        <center> <span style="color:gray;"><b>Challenge</b></span> <br>\
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Eval Datasets</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Eval Metrics</b></span> <br>
                    </td>
                    <td width=100>
                        <center> <span style="color:gray;"><b>Instructions</b></span> <br>
                    </td>                            
                    <td width=170>
                        <center> <span style="color:black;"><b>Make a Challenge Submission</b>
                            <img src="./static/eccv2022/img/icons/icon_live.png" class=" style=" width="50" height="40" ;>
                        </span> <br>
                    </td>                            

                </tr>
                <tr>
                    <td width=30>
                        <center> <span><b>ICinW</b></span></center>
                    </td>
                    <td width=150>
                        <center> <span><b>20 Image Classification Datasets</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><b>Zero, few, full-shot</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><a href="https://github.com/Computer-Vision-in-the-Wild/Elevater_Toolkit_IC"> <img src="./static/eccv2022/img/icons/instruction.png" class="style=" width="50" height="50" ;> </a></span></center>
                    </td>     
                    <td width=170>
                        <center> <span><a href="https://eval.ai/web/challenges/challenge-page/1832/overview"> <img src="https://evalai.s3.amazonaws.com/media/logos/0edb5d9a-27cc-42d0-b548-8d8a5d268c5b.jpg" class="home_team_picture style=" width="100" height="100" ;> </a></span></center>
                    </td>                                                        
                </tr>
                <tr>
                    <td width=30>
                        <center> <span><b>ODinW</b></span></center>
                    </td>
                    <td width=150>
                        <center> <span><b>35 Object Detection Datasets</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><b>Zero, few, full-shot</b></span></center>
                    </td>
                    <td width=100>
                        <center> <span><a href="https://github.com/microsoft/GLIP#the-object-detection-in-the-wild-benchmark"> <img src="./static/eccv2022/img/icons/instruction.png" class=" style=" width="50" height="50" ;> </a></span></center>
                    </td>  
                    <td width=170>
                        <center> <span><a href="https://eval.ai/web/challenges/challenge-page/1839/overview"> <img src="https://evalai.s3.amazonaws.com/media/logos/b8a06ae4-2172-43ae-b65f-64c6139d807c.jpg" class="home_team_picture style=" width="100" height="100" ;> </a></span></center>
                    </td>  
                </tr>

                </tr>
            </table>
            
            
            To prevent a race purely in pre-training data and model size, we will have two tracks.
            
            
            <li style="margin-left:30px">
                
                For the academic track, pre-training data is limited: 
                    (1) ICinW: 
                <a href="https://www.image-net.org/">ImageNet21K (Removing ImageNet1K)</a>, 
                <a href="https://github.com/google-research-datasets/conceptual-captions">CC3M</a>+<a href="https://www.objects365.org/overview.html">CC12M</a>, <a href="https://github.com/openai/CLIP/blob/main/data/yfcc100m.md">YFCC15M</a>; 
                    (2) ODinW: <a href="https://www.objects365.org/overview.html">Objects365</a>; 
                    (3) SGinW: <a href="https://cocodataset.org/">COCO</a>, <a href="https://github.com/mjhucla/Google_Refexp_toolbox">RefCOCO-g</a>.


                
            </li>

            <li style="margin-left:30px">
                For the industry track, there is no limitation on pre-training data and model size. Teams are required to disclose meta info of model and data if extra data is used. Here are some publicly available image-text datasets: (1) FLAVA <a href="https://huggingface.co/datasets/facebook/pmd">Public Multimodal Datasets (PMD)</a> corpus with 70M pairs; (2) <a href="https://laion.ai/projects/">LAION</a> with 400M or 5B pairs.


            </li>

            <br>
            Please see the submission pages for detailed requirements in each Challenge -> Track -> Phase. More information about the challenge benchmark is released: 
            <a href="https://computer-vision-in-the-wild.github.io/ELEVATER/"> [Benchmark] </a>
            <a href="https://arxiv.org/abs/2204.08790"> [Document]</a>
                <a href="https://github.com/Computer-Vision-in-the-Wild/DataDownload"> [Data Download]</a>.
            Please reach out if you have any issue in submissions.
            <br><br>

            
        </p>
    </div>
    <hr> 
</div> -->



    <div class="row">
        <h1 style="font-size:30px; color:grey; font-weight: 200">Dates</h1>
        <div class="large-12 columns" style="text-align:left;">
            <div class="large-12 columns" style="text-align:left;">
                <p style="font-size:15px; font-weight: 200; border-style: solid;
                                border-width: 1px; text-align:justify; padding:5px; width:85%">
                    <code>
                            <span style="width:40%; margin:5px; display:inline-block;">Feb, 2024</span>
                            <span style="display:inline-block; margin-left:10px;">Competition starts, testing phase begins</span><br>
    
                            <span style="width:40%; margin:5px; display:inline-block;">
                                June 2nd, 2024
                            </span>
                            <span style="display:inline-block; margin-left:10px;">Competition ends (challenge paper submission)</span><br>
    
                            <span style="width:40%; margin:5px; display:inline-block;">
                            April 28st, 2024
    <!--                                         <a href="https://cmt3.research.microsoft.com/CVinW2024"> <img src="./static/eccv2022/img/icons/icon_news.png" class=" style=" width="40" height="40" ;> </a> -->
                        </span>
                            <span style="display:inline-block; margin-left:10px;">Workshop paper submission deadline</span><br>
    
                            <span style="width:40%; margin:5px; display:inline-block;">May 26th, 2024</span>
                            <span style="display:inline-block; margin-left:10px;">Workshop paper acceptance decision to authors</span><br>
    
    
                            <span style="width:40%; margin:5px; display:inline-block;">June 2nd, 2024</span>
                            <span style="display:inline-block; margin-left:10px;">Camera-ready submission deadline</span><br>
    
                        </code></p>
            </div>
        </div>
        <hr>
    </div>

<div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Workshop Organizers</h1>
    <div class="team" id="people" align=center>
        <div class="row">


            <div class="large-1 columns">
                <a href="https://chunyuan.li/"><img src="https://vlp-tutorial.github.io/2022/img/organizer/chunyuan.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Chunyuan Li
                    <br>ByteDance / TikTok</p>
            </div>

            <div class="large-1 columns">
                <a href="https://jwyang.github.io"><img src="https://jwyang.github.io/images/profile.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jianwei Yang
                    <br>Microsoft</p>
                </div>



                
            <div class="large-1 columns">
                <a href="https://hliu.cc/"><img src="./static/eccv2022/img/haotian-circle.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Haotian Liu
                    <br>UW Madison</p>
            </div>

            <div class="large-1 columns">
                <a href="https://maureenzou.github.io/"><img src="./static/cvpr2023/img/organizers/xueyan.jfif" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Xueyan Zou
                    <br>UW Madison
                </p>
            </div>


            <div class="large-1 columns">
                <a href="https://wanrong-zhu.com/"><img src="https://wanrong-zhu.com/author/wanrong-zhu/avatar_huac9fc6b92b4eff9f49f0fa2aadd3d764_1202942_270x270_fill_q90_lanczos_center.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Wanrong Zhu 
                    <br>UCSB</p>
            </div>             


            <div class="large-1 columns">
                <a href="https://yonatanbitton.github.io/"><img src="https://yonatanbitton.github.io/authors/admin/avatar_hub6a82f0e24dfc8eb87466fed56bca441_709630_270x270_fill_q75_lanczos_center.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yonatan Bitton
                    <br>Google</p>
            </div>

            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/jfgao/"><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/09/20191101_170350330_iOS-4.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jianfeng Gao
                    <br>Microsoft</p>
            </div> 

<!--             
            <div class="large-1 columns">
                <a href="http://jyotianeja.com/"><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=FYB92lkAAAAJ&citpid=1" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jyoti Aneja
                    <br>Microsoft</p>
            </div>


            <div class="large-1 columns">
                    <a href="https://xinw.ai/"><img src="https://xinw.ai/assets/images/photo.JPG" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Xin Wang
                        <br>Microsoft</p>
            </div>
  
            <div class="large-1 columns">
                <a href="https://liunian-harold-li.github.io/"><img src="https://liunian-harold-li.github.io/img/harold_cropped.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Liunian Li
                    <br>UCLA</p>
            </div>

            <div class="large-1 columns">
                <a href="https://pzzhang.github.io/pzzhang/"><img src="./static/eccv2022/img/pengchuan-circle.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Pengchuan Zhang
                    <br>Meta AI</p>
            </div>

            <div class="large-1 columns">
                <a href="https://ashkamath.github.io/"><img src="./static/eccv2022/img/aishwarya_k.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Aishwarya Kamath
                    <br>NYU</p>
            </div>  -->
        <!-- </div> -->
    <hr>
</div>
</div>



<!-- <div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Challenge Organizers and Participants</h1>
    <div class="team" id="people" align=center>
        <div class="row">



             <div class="large-1 columns">
                <a href="https://github.com/FrancescoSaverioZuppichini"><img src="./static/cvpr2024/img/organizers/Francesco.png" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Francesco Zuppichini
                    <br>Roboflow
                </p>
            </div> 


            <div class="large-1 columns">
                <a href="https://fengli-ust.github.io/"><img src="./static/cvpr2024/img/organizers/Feng_Li.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Feng Li
                    <br>HKUST
                </p>
            </div>
            
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=en"><img src="./static/cvpr2024/img/organizers/Hao_Zhang.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Hao Zhang
                    <br>HKUST
                </p>
            </div>

            <div class="large-1 columns">
                <a href="https://rentainhe.github.io/"><img src="./static/cvpr2024/img/organizers/tianhe.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Tianhe Ren
                    <br>IDEA
                </p>
            </div>

            <div class="large-1 columns">
                <a href="https://www.lsl.zone/"><img src="./static/cvpr2024/img/organizers/Shilong_Liu.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Shilong Liu
                    <br>Tsinghua University
                </p>
            </div>

            <div class="large-1 columns">
                <a href="https://www.lsl.zone/"><img src="./static/cvpr2024/img/organizers/Jiarui_Xu.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jiarui Xu
                    <br>UCSD
                </p>
            </div>

            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=p9EoCKAAAAAJ&hl=zh-CN"><img src="./static/cvpr2024/img/organizers/Jiannan_Wu.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jiannan Wu
                    <br>HKU
                </p>
            </div>

            <div class="large-1 columns">
                <a href="https://masterbin-iiau.github.io/"><img src="./static/cvpr2024/img/organizers/Bin_Yan.jpeg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Bin Yan
                    <br>DUT
                </p>
            </div>

            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=C04zJHEAAAAJ&hl=en"><img src="./static/cvpr2024/img/organizers/Mengde_Xu.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Mengde Xu
                    <br>HUST
                </p>
            </div>

            <div class="large-1 columns">
                <a href="https://stupidzz.github.io/"><img src="./static/cvpr2024/img/organizers/Zheng_Zhang.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Zheng Zhang
                    <br>MSRA
                </p>
            </div>



</div>
</div> 
 -->

<!-- Challenge Organizers -->



<!-- <div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Challenge Organizers</h1>
    <div class="team" id="people">
        <div class="row">
            <div class="large-1 columns">
                <a href="https://sites.google.com/site/yinfeiyang"><img src="./static/eccv2022/img/yinfei_yang.jpeg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yinfei Yang 
                    <br>Apple</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/yi-ting-chen-google/ "><img src="./static/eccv2022/img/yiting_chen.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yi-Ting Chen 
                    <br>Google</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/ye-xia-19b46b42/ "><img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=QQhJ1pAAAAAJ&citpid=1" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Ye Xia 
                    <br>Google</p>
            </div>

            <div class="large-1 columns">
                <a href="https://openreview.net/profile?id=~Yangguang_Li1 "><img src="./static/eccv2022/img/YangguangLi.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yangguang Li 
                    <br>Sensetime</p>
            </div>
            <div class="large-1 columns">
                    <a href="https://jeff-liangf.github.io"><img src="./static/eccv2022/img/feng_liang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Feng Liang 
                        <br>UT Austin</p>
            </div>
            <div class="large-1 columns">
                <a href="https://slothercui.github.io/ "><img src="./static/eccv2022/img/YufengCui.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yufeng Cui 
                    <br>Sensetime</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/pingjin1"><img src="./static/eccv2022/img/ping_jin.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Ping Jin
                    <br>Microsoft</p>
            </div>   
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/shohei-ono-ab7404a8/"><img src="./static/eccv2022/img/shohei.png" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Shohei Ono 
                    <br>Microsoft</p>
            </div> 
            <div class="large-1 columns">
                <a href="https://houwenpeng.com/"><img src="https://www.microsoft.com/en-us/research/uploads/prod/2019/07/Houwen_Peng_360x360-5d1ce02ab189f.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Houwen Peng
                    <br>Microsoft</p>
            </div> 
            <div class="large-1 columns">
                <a href="https://www.sainingxie.com/"><img src="./static/eccv2022/img/saining.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Saining Xie
                        <br>NYU/Meta</p>
            </div>
            <div class="large-1 columns">
                <a href="https://ancientmooner.github.io/"><img src="./static/eccv2022/img/han_hu.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Han Hu
                        <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
            </div>            
        </div>

        <div class="row">
            <div class="large-1 columns">
                <a href="https://apsdehal.in/"><img src="./static/eccv2022/img/aman.jpeg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Amanpreet Singh
                    <br>HuggingFace</p>
            </div>
            <div class="large-1 columns">
                <a href="https://scholar.google.com.sg/citations?user=OEZ816YAAAAJ&hl=en"><img src="./static/eccv2022/img/xiaojie_jin.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Xiaojie Jin
                        <br>Bytedance</p>
            </div>
            <div class="large-1 columns">
                <a href="https://sites.google.com/site/jshfeng/"><img src="./static/eccv2022/img/jiashi_feng.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Jiashi Feng
                        <br>Bytedance</p>
            </div>
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=qp6IwtgAAAAJ&hl=zh-CN"><img src="./static/eccv2022/img/junyang_lin.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Junyang Lin
                        <br>Alibaba</p>
                </div>
              
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=vO9FZekAAAAJ&hl=zh-CN"><img src="./static/eccv2022/img/an_yang.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">An Yang
                        <br>Alibaba</p>
                </div>

            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=7fjqA0YAAAAJ"><img src="./static/eccv2022/img/peng_wang.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Peng Wang
                        <br>Alibaba</p>
                </div>

            <div class="large-1 columns">
                <a href="https://nguyenbh.github.io/"><img src="https://nguyenbh.github.io/authors/admin/avatar_huaf86694975fe5beecb493d38fc677dd1_22672_270x270_fill_q90_lanczos_center.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Nguyen Bach
                        <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="https://github.com/lynlynlyn"><img src="./static/eccv2022/img/yuning_lu.jpg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Yuning Lu
                        <br>USTC</p>
            </div>    

            <div class="large-1 columns">
                <a href="https://zhangyuanhan-ai.github.io/"><img src="./static/eccv2022/img/yuanhan_zhang.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Yuanhan Zhang
                        <br>NTU</p>
                </div>            

            <div class="large-1 columns">
                <a href="https://kaiyangzhou.github.io/"><img src="https://kaiyangzhou.github.io/images/ky.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Kaiyang Zhou
                        <br>NTU</p>
                </div>

            <div class="large-1 columns">
                <a href="https://liuziwei7.github.io/"><img src="./static/eccv2022/img/ziwei_liu.png" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Ziwei Liu
                        <br>NTU</p>
            </div>

            <div class="large-1 columns">
            </div>
        </div>

        <div class="row">
            <div class="large-1 columns">
                <a href="https://www.lsl.zone/"><img src="./static/eccv2022/img/shilong_liu.jpeg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Shilong Liu
                    <br>Tsinghua University</p>
            </div>
            <div class="large-1 columns">
                <a href="https://fengli-ust.github.io/"><img src="./static/eccv2022/img/feng_li.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Feng Li
                        <br>HKUST</p>
            </div>
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=zh-CN"><img src="./static/eccv2022/img/hao_zhang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Hao Zhang
                        <br>HKUST</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/jianfengwang1"><img src="./static/eccv2022/img/jianfeng_wang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Jianfeng Wang
                        <br>Microsoft</p>                
            </div>
              
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/lijuanw/"><img src="./static/eccv2022/img/lijuan_wang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Lijuan Wang
                        <br>Microsoft</p>                
            </div>
            
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=kDzxOzUAAAAJ&hl=en"><img src="./static/eccv2022/img/xuehai_he.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Xuehai He
                        <br>UCSC</p>                
            </div>
            
            <div class="large-1 columns">
                <a href="https://eric-xw.github.io/"><img src="./static/eccv2022/img/xin_wang.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Xin Eric Wang
                        <br>UCSC</p>                
            </div>

            <div class="large-1 columns">
                <a href="https://cse.buffalo.edu/~changyou/"><img src="./static/eccv2022/img/changyou_chen.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Changyou Chen
                        <br>University at Buffalo, SUNY</p>                   
            </div>

            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/yeahgoyixu"><img src="./static/eccv2022/img/yi_xu.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Yi Xu
                        <br>Amazon</p>                   
            </div>
            
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en"><img src="./static/eccv2022/img/haoxuan_you.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                        <br><br>
                    </a>
                    <p style="font-size:12px; font-weight: 200;">Haoxuan You
                        <br>Columbia University</p>                  
            </div>    


            <div class="large-1 columns">
            </div>            

            <div class="large-1 columns">
            </div>

            <div class="large-1 columns">
            </div>

            <div class="large-1 columns">
            </div>
        </div>

        </div>  

    </div>      


    <hr>    
</div> -->




<!-- <div class="row">
    <h1 style="font-size:30px; color:grey; font-weight: 200">Advisory Committee</h1>
    <div class="team" id="people">
        <div class="row">
            <div class="large-1 columns">
                <a href="https://people.eecs.berkeley.edu/~trevor/"><img src="https://www2.eecs.berkeley.edu/Faculty/Photos/Fullsize/darrell.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Trevor Darrell
                    <br>UC Berkley</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.leizhang.org/"><img src="./static/eccv2022/img/leizhang-circle.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Lei Zhang
                    <br>IDEA</p>
            </div>
            <div class="large-1 columns">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/"><img src="./static/eccv2022/img/lee-circle.jpg"" target="_blank" class="home_team_picture style=" width="75" height="75"  ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yong Jae Lee
                    <br>UW Madison</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.linkedin.com/in/houdong-hu-08334227"><img src="./static/eccv2022/img/Houdong_profile.png" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Houdong Hu
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/zliu/"><img src="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/03/avatar_user__1490216903-360x360.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Zicheng Liu
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="http://people.csail.mit.edu/celiu/"><img src="https://scholar.googleusercontent.com/citations?view_op=view_photo&user=j7MW4iYAAAAJ&citpid=8" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Ce Liu
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/xdh/"><img src="./static/eccv2022/img/xuedong-circle.jpg" target="_blank" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Xuedong Huang
                    <br>Microsoft</p>
            </div>
            <div class="large-1 columns">
                <a href="http://web.cs.ucla.edu/~kwchang"><img src="./static/eccv2022/img/kaiwei-circle.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Kai-Wei Chang
                    <br>UCLA</p>
            </div>
            <div class="large-1 columns">
                <a href="https://jingdongwang2017.github.io/"><img src="./static/eccv2022/img/WangJingdong.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jingdong Wang
                    <br>Baidu</p>
            </div>  
            <div class="large-1 columns">
                <a href="https://pages.ucsd.edu/~ztu/"><img src="./static/eccv2022/img/ztu_09.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Zhuowen Tu
                    <br>UCSD</p>
            </div>  
            <div class="large-1 columns">
                <a href="https://www.microsoft.com/en-us/research/people/jfgao/"><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/09/20191101_170350330_iOS-4.jpg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jianfeng Gao
                    <br>Microsoft</p>
            </div>                
            <div class="large-1 columns">
            </div>
        </div>

        <div class="row">
            <div class="large-1 columns">
            </div>
            <div class="large-1 columns">
                <a href="https://people.ece.uw.edu/hwang/"><img src="./static/eccv2022/img/hwang-circle.jpg" target="_blank" class="home_team_picture style=" width="70" height="70" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Jenq-Neng Hwang
                    <br>University of Washington</p>
            </div>  
            <div class="large-1 columns">
                <a href="https://scholar.google.com/citations?user=WLN3QrAAAAAJ&hl=en"><img src="./static/eccv2022/img/lecun.jpeg" class="home_team_picture style=" width="75" height="75" ;>
                    <br><br>
                </a>
                <p style="font-size:12px; font-weight: 200;">Yann LeCun
                    <br>NYU/Meta</p>
            </div> 
            <hr>
        </div>        
        <div class="row">
            <div class="disclaimer">
                Disclaimer: To ensure fair comparisons in the challenge, the evaluation server and leaderboards are independently developed and maintained by the <b>Workshop Organizers</b>, while the <b>Challege Organizers</b> actively promote and contribute to the competitions.
            </div>
        </div>
        <hr>
    </div>
</div> -->

        <!-- Contact Information -->
        <div class="large-12 columns" style="background: white;">
            <p style="color:black; text-align:center; display:block; margin-top:7px;">Workshop and Challenge Questions?
                <br>
                Reach out: <a href="https://computer-vision-in-the-wild.github.io/cvpr-2024/" target="_top">https://github.com/Computer-Vision-in-the-Wild/cvpr-2024</a>
                <br>
                Workshop Organizing Team
            </p>
        </div>
    </section>
    <script>
    $(document).foundation();
    </script>
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
            m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-63638588-1', 'auto');
    ga('send', 'pageview');
    </script>
    <!-- jquery smooth scroll to id's -->
    <script>
    $(function() {
        $('a[href*=#]:not([href=#])').click(function() {
            if (location.pathname.replace(/^\//, '') == this.pathname.replace(/^\//, '') && location.hostname == this.hostname) {
                var target = $(this.hash);
                target = target.length ? target : $('[name=' + this.hash.slice(1) + ']');
                if (target.length) {
                    $('html,body').animate({
                        scrollTop: target.offset().top
                    }, 1000);
                    return false;
                }
            }
        });
    });
    </script>
    <script>
        $(".abstract-toggle").click(function () {

        $header = $(this);
        //getting the next element
        $content = $header.next();
        //open up the content needed - toggle the slide- if visible, slide up, if not slidedown.
        $content.slideToggle(500, function () {
            //execute this after slideToggle is done
            //change text of header based on visibility of content div
            $header.text(function () {
                //change text based on condition
                return $content.is(":visible") ? "[Collapse]" : "[Expand]";
            });
        });

        });
    </script>
</body>

</html>
